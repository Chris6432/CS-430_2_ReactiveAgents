\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{epigraph}
\usepackage{amssymb}
\usepackage{mathtools}

%% declaring abs so that it works nicely
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

% Marges
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=5.5in
\textheight=9.0in
\headsep=0.5in


\title{Pickup and Delivery Reactive Agent: Implementation of MDP}
\date{\today}
\author{Christophe Marciot \& Titouan Renard}

\begin{document}
\maketitle	

\section{Problem definition}

\subsection{Markov Decision Process}

In a MDP current state is know with certainty, but the reward of transition is not. A MDP is defined by : 

\begin{align*}
    \textit{A reward function:} && \overbrace{R(s,a) \rightarrow \mathbb{R}}^{\textit{Where $s$ denotes a state and $a$ an action}} \\
    \textit{A probabilistic state transition table:} && \overbrace{T(s,a,s') = p(s'|s,a)}^{\textit{Where $s'$ denotes the state the action leads to}}
\end{align*}

The goal of the process is to find a policy $\pi$ such that \textit{the average reward is maximized}.

\subsection{The Pickup and Delivery Problem}

Agents exist in a static environment (a model of Switzerland's road network) described by a graph. Nodes of the graph are called \textit{cities} and it's (weighted) edges are called \textit{roads}. \\

The pickup and delivery problem is described by a series of tasks spread over the topology, the transportation tasks are described by: 

\begin{enumerate}
    \item Pickup city (and it's position)
    \item Delivery city (and it's position)
    \item Reward in CHF
\end{enumerate}

\newpage

\subsection{Definitions}

\subsubsection{Existing tables}

The dataset usable for learning is described by two probability tables :

\begin{enumerate}
    \item $P_{table}(i,j)$ : the probability of a task for city $j$ to be present in city $i$
    \item $R_{table}(i,j)$ : the average reward given when a task is transported from city $i$ to city $j$
\end{enumerate}

\subsubsection{State} 
It doesn't seem to make sense for the state to be anything other than \textbf{the city the agent is when it has no task}, this is because :
\begin{enumerate}
    \item An agent can not have more than one task
    \item When an agent has a task it cannot interrupt it
    \item There is no difference between an agent in a city with no task because it succeeded or because it failed it's last task, it still has to make a decision about how to get another task
\end{enumerate}

The set $S$ containing all states is exactly the set of all cities.

\subsubsection{Action}
An action consists in the agent either:
\begin{enumerate}
    \item going to another city with no tax
    \item taking a task in the city it's in (which amounts to moving to another city)
\end{enumerate}
And always result in the agent being in a city (different or not from it's starting point, the agent can loop between two city if it maximizes reward) without a task, in another words in a (new) state. \\

The set $A$ containing all actions is the set of all pairs of two cities on the network.
\subsubsection{Reward}

Given a state $s$ the agent is in and for and action $a$, the reward function for a single action is defined as follows :

\[R_{action}(s,a) = \begin{cases}
    0               & \textit{if the agent moves to a city without taking a contract} \\
    R_{table}(i(a),j(a))    & \textit{if the agent moves to a city after taking a contract}
\end{cases}\]

\subsubsection{Reward}

Where  $i(a)$ is the starting city of the action $a$ and $j(a)$ it's ending city, note that $i(a) = s$.

\subsubsection{Discount factor}

We have, in order to ensure the convergence of $V(s)$ that :

\[ \gamma \in [0,1[ \]

We want our system to optimize for time, not for number of actions take. Because an action may take more or less time. So let's define our $\gamma$ as a function of an action :

\[\gamma (a) = \frac{Â \min_{\alpha \in \text{ actions}} (t(\alpha))}{t(a)}\]

\subsubsection{Probability of transition $p(s'|s,a)$}


\newpage

\section{Solving the MDP}

We denote \textit{the value of a state} $s$ as $V(s)$. This value represents "\textit{the potential rewards from this state onwards}". In order to ensure $V(s_i) < \infty$ $\forall i$ (and make the problem solvable) we introduce a \textit{discount factor} $\gamma \in [0 ... 1 [$.

\[V(s_i) = R(s_i) + \gamma \cdot V(T(s_i),a(s_i))\]



\end{document}